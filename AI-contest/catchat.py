import os
import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from pypdf import PdfReader
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import pandas as pd

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì—°ì„¸ëŒ€ ìš”ëŒë„ìš°ë¯¸ ìš”ëŒì¡°ëŒ",
    page_icon="https://www.yonsei.ac.kr/sites/sc/images/sub/img-symbol6.png",
    layout="centered",
)


# ê³ ì • PDF ê²½ë¡œ
PDF_PATH = "YonseiUniversityCatalog.pdf"

# ì§€ì •ëœ ì›¹í˜ì´ì§€ ë§í¬ ëª©ë¡ (ì‚¬ìš©ìê°€ ì§ì ‘ ì¶”ê°€)
SPECIFIED_URLS = [
    "https://www.yonsei.ac.kr/sc/275/subview.do",
    "https://www.yonsei.ac.kr/sc/276/subview.do",
    "https://www.yonsei.ac.kr/sc/277/subview.do",
    "https://www.yonsei.ac.kr/sc/278/subview.do",
    "https://www.yonsei.ac.kr/sc/279/subview.do",
    "https://www.yonsei.ac.kr/sc/386/subview.do",
    "https://www.yonsei.ac.kr/sc/387/subview.do",
    "https://www.yonsei.ac.kr/sc/281/subview.do",
    "https://www.yonsei.ac.kr/sc/376/subview.do",
    "https://www.yonsei.ac.kr/sc/377/subview.do",
    "https://www.yonsei.ac.kr/sc/378/subview.do",
    "https://www.yonsei.ac.kr/sc/379/subview.do",
    "https://www.yonsei.ac.kr/sc/383/subview.do",
    "https://www.yonsei.ac.kr/sc/384/subview.do",
    "https://www.yonsei.ac.kr/sc/385/subview.do",
    "https://www.yonsei.ac.kr/sc/301/subview.do",
    "https://www.yonsei.ac.kr/sc/254/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGc2MlMkY1OCUyRjk0MjA3OCUyRmFydGNsVmlldy5kbyUzRnBhZ2UlM0QxJTI2ZmluZFR5cGUlM0QlMjZmaW5kV29yZCUzRCUyNmZpbmRDbFNlcSUzRCUyNmZpbmRPcG53cmQlM0QlMjZyZ3NCZ25kZVN0ciUzRCUyNnJnc0VuZGRlU3RyJTNEJTI2cGFzc3dvcmQlM0QlMjY%3D",
    "https://www.yonsei.ac.kr/sc/254/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGc2MlMkY1OCUyRjk0MjA2NCUyRmFydGNsVmlldy5kbyUzRnBhZ2UlM0QxJTI2ZmluZFR5cGUlM0QlMjZmaW5kV29yZCUzRCUyNmZpbmRDbFNlcSUzRCUyNmZpbmRPcG53cmQlM0QlMjZyZ3NCZ25kZVN0ciUzRCUyNnJnc0VuZGRlU3RyJTNEJTI2cGFzc3dvcmQlM0QlMjY%3D",
    "https://www.yonsei.ac.kr/sc/254/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGc2MlMkY1OCUyRjk0MjA2MiUyRmFydGNsVmlldy5kbyUzRnBhZ2UlM0QxJTI2ZmluZFR5cGUlM0QlMjZmaW5kV29yZCUzRCUyNmZpbmRDbFNlcSUzRCUyNmZpbmRPcG53cmQlM0QlMjZyZ3NCZ25kZVN0ciUzRCUyNnJnc0VuZGRlU3RyJTNEJTI2cGFzc3dvcmQlM0QlMjY%3D",
    "https://www.yonsei.ac.kr/sc/254/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGc2MlMkY1OCUyRjk0MjA2MSUyRmFydGNsVmlldy5kbyUzRnBhZ2UlM0QxJTI2ZmluZFR5cGUlM0QlMjZmaW5kV29yZCUzRCUyNmZpbmRDbFNlcSUzRCUyNmZpbmRPcG53cmQlM0QlMjZyZ3NCZ25kZVN0ciUzRCUyNnJnc0VuZGRlU3RyJTNEJTI2cGFzc3dvcmQlM0QlMjY%3D",
    "https://www.yonsei.ac.kr/sc/254/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGc2MlMkY1OCUyRjkyMzI1OCUyRmFydGNsVmlldy5kbyUzRnBhZ2UlM0QxJTI2ZmluZFR5cGUlM0RzaiUyNmZpbmRXb3JkJTNEJUVBJUI1JUIwJUVBJUIwJTk1JUVDJUEyJThDJTI2ZmluZENsU2VxJTNEJTI2ZmluZE9wbndyZCUzRCUyNnJnc0JnbmRlU3RyJTNEJTI2cmdzRW5kZGVTdHIlM0QlMjZwYXNzd29yZCUzRCUyNg%3D%3D",
    "https://www.yonsei.ac.kr/sc/254/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGc2MlMkY1OCUyRjk0MTIwMCUyRmFydGNsVmlldy5kbyUzRnBhZ2UlM0QxJTI2ZmluZFR5cGUlM0RzaiUyNmZpbmRXb3JkJTNEJUVBJUI1JUIwJUVBJUIwJTk1JUVDJUEyJThDJTI2ZmluZENsU2VxJTNEJTI2ZmluZE9wbndyZCUzRCUyNnJnc0JnbmRlU3RyJTNEJTI2cmdzRW5kZGVTdHIlM0QlMjZwYXNzd29yZCUzRCUyNg%3D%3D",
    "https://libart.yonsei.ac.kr/libart/degree/requirements_10.do",
    "https://computing.yonsei.ac.kr/sub3_1.php",
    "https://swedu.yonsei.ac.kr/yonseisw/swedu02.do",
    "https://swedu.yonsei.ac.kr/yonseisw/swedu01.do",
    "https://swedu.yonsei.ac.kr/yonseisw/swedu03.do",
    "https://universitycollege.yonsei.ac.kr/fresh/refinement/goal.do",
    "https://yicrc.yonsei.ac.kr/main/rc.asp?mid=m01_06",
    "https://yicrc.yonsei.ac.kr/main/rc.asp?mid=m01_04",
    "https://yicrc.yonsei.ac.kr/main/rc.asp?mid=m01_01",
    "https://www.yonsei.ac.kr/sc/285/subview.do",
    "https://www.yonsei.ac.kr/sc/286/subview.do",
    "https://www.yonsei.ac.kr/sc/388/subview.do",
    "https://ihei.yonsei.ac.kr/ihei/Program/program_whole.do",
    "https://ihei.yonsei.ac.kr/ihei/innovation/innovation_program.do",
    "https://oia.yonsei.ac.kr/partner/chStu.asp",
    "https://oia.yonsei.ac.kr/partner/chStu2.asp",
    "https://oia.yonsei.ac.kr/partner/chStu3.asp",
    "https://oia.yonsei.ac.kr/partner/chStu4.asp",
    "https://oia.yonsei.ac.kr/partner/chStu5.asp",
    "https://oia.yonsei.ac.kr/partner/chStu6.asp",
    "https://oia.yonsei.ac.kr/partner/chStu7.asp",
    "https://oia.yonsei.ac.kr/partner/chStu10.asp",
    "https://oia.yonsei.ac.kr/partner/chStu11.asp",
    "https://oia.yonsei.ac.kr/partner/grade.asp"
]
    


def load_pdf_docs(pdf_path: str):
    """pypdfë¡œ PDF ë¬¸ì„œë¥¼ ì½ì–´ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    reader = PdfReader(pdf_path)
    docs = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        if not text.strip():
            continue
        docs.append(
            Document(
                page_content=text,
                metadata={
                    "page": i + 1,
                    "source": pdf_path,
                    "source_type": "pdf",
                },
            )
        )
    return docs


def split_docs(docs):
    """ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2500,
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_documents(docs)


def extract_tables_from_html(html_content: str, base_url: str) -> str:
    """HTMLì—ì„œ í‘œë¥¼ ì¶”ì¶œí•˜ì—¬ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    soup = BeautifulSoup(html_content, 'html.parser')
    tables_text = []
    
    for i, table in enumerate(soup.find_all('table')):
        try:
            # pandasë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œë¥¼ ì½ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            dfs = pd.read_html(str(table))
            if dfs:
                table_text = f"\n[í‘œ {i+1}]\n"
                table_text += dfs[0].to_string(index=False)
                tables_text.append(table_text)
        except Exception as e:
            # pandasë¡œ ì½ê¸° ì‹¤íŒ¨ ì‹œ BeautifulSoupìœ¼ë¡œ ì§ì ‘ ì¶”ì¶œ
            table_text = f"\n[í‘œ {i+1}]\n"
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if cells:
                    row_text = ' | '.join(cell.get_text(strip=True) for cell in cells)
                    table_text += row_text + '\n'
            if table_text.strip():
                tables_text.append(table_text)
    
    return '\n'.join(tables_text)


def extract_images_from_html(html_content: str, base_url: str) -> str:
    """HTMLì—ì„œ ì´ë¯¸ì§€ ë° ì¸í¬ê·¸ë˜í”½ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, 'html.parser')
    images_info = []
    
    for img in soup.find_all('img'):
        img_src = img.get('src', '')
        img_alt = img.get('alt', '')
        img_title = img.get('title', '')
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if img_src:
            img_url = urljoin(base_url, img_src)
            
            img_info = f"[ì´ë¯¸ì§€: {img_alt or img_title or 'ì´ë¯¸ì§€'}]"
            if img_alt:
                img_info += f" ì„¤ëª…: {img_alt}"
            if img_title and img_title != img_alt:
                img_info += f" ì œëª©: {img_title}"
            images_info.append(img_info)
    
    return '\n'.join(images_info)


def load_web_docs(urls: list[str]):
    """ì›¹í˜ì´ì§€ë“¤ì„ ë¡œë“œí•˜ì—¬ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í‘œì™€ ì´ë¯¸ì§€ í¬í•¨)"""
    if not urls:
        return []
    
    docs = []
    failed_urls = []
    success_count = 0
    
    def load_single_url(url):
        try:
            # User-Agent í—¤ë” ì¶”ê°€
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # HTML ì›ë³¸ ê°€ì ¸ì˜¤ê¸° (í‘œì™€ ì´ë¯¸ì§€ ì¶”ì¶œìš©)
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            html_content = response.text
            
            # WebBaseLoaderë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¡œë“œ
            loader = WebBaseLoader(url)
            loaded_docs = loader.load()
            
            # í‘œ ì¶”ì¶œ
            tables_text = extract_tables_from_html(html_content, url)
            
            # ì´ë¯¸ì§€/ì¸í¬ê·¸ë˜í”½ ì •ë³´ ì¶”ì¶œ
            images_text = extract_images_from_html(html_content, url)
            
            # ê° ë¬¸ì„œì— í‘œì™€ ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            valid_docs = []
            for doc in loaded_docs:
                enhanced_content = doc.page_content
                
                # ë¹ˆ ì½˜í…ì¸  í•„í„°ë§
                if not enhanced_content or len(enhanced_content.strip()) < 50:
                    continue
                
                # í‘œê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if tables_text:
                    enhanced_content += f"\n\n=== í‘œ ì •ë³´ ===\n{tables_text}"
                
                # ì´ë¯¸ì§€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if images_text:
                    enhanced_content += f"\n\n=== ì´ë¯¸ì§€ ì •ë³´ ===\n{images_text}"
                
                doc.page_content = enhanced_content
                doc.metadata.update({
                    "source": url,
                    "source_type": "web",
                    "content_length": len(doc.page_content),
                })
                valid_docs.append(doc)
            
            if not valid_docs:
                failed_urls.append((url, "Empty or too short content"))
            
            return valid_docs
        except Exception as e:
            error_msg = str(e)
            failed_urls.append((url, error_msg))
            print(f"Error loading {url}: {error_msg}")
            return []
    
    # ë³‘ë ¬ë¡œ ì›¹í˜ì´ì§€ ë¡œë“œ (ì†ë„ í–¥ìƒ)
    with ThreadPoolExecutor(max_workers=3) as executor:  # ì›Œì»¤ ìˆ˜ ì¡°ì • (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        future_to_url = {executor.submit(load_single_url, url): url for url in urls}
        for future in as_completed(future_to_url):
            try:
                loaded_docs = future.result(timeout=30)  # íƒ€ì„ì•„ì›ƒ ì¶”ê°€
                if loaded_docs:
                    docs.extend(loaded_docs)
                    success_count += 1
            except Exception as e:
                url = future_to_url[future]
                failed_urls.append((url, f"Timeout or error: {str(e)}"))
                print(f"Error processing {url}: {e}")
    
    # ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
    print(f"\n{'='*70}")
    print(f"Web Loading Summary:")
    print(f"  Total URLs: {len(urls)}")
    print(f"  Success: {success_count}")
    print(f"  Failed: {len(failed_urls)}")
    print(f"  Total Documents: {len(docs)}")
    
    if failed_urls:
        print(f"\nFailed URLs ({len(failed_urls)}):")
        for url, error in failed_urls[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            print(f"  âœ— {url}")
            print(f"    â†’ {error[:80]}")
        if len(failed_urls) > 10:
            print(f"  ... and {len(failed_urls) - 10} more")
    print(f"{'='*70}\n")
    
    return docs


def format_docs(docs):
    """retriever ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ í¬ë§·"""
    lines = []
    for d in docs:
        src_type = d.metadata.get("source_type", "unknown")
        page = d.metadata.get("page")
        prefix = f"[{src_type.upper()}]"
        if page:
            prefix += f"[p{page}]"
        lines.append(f"{prefix} {d.page_content}")
    return "\n\n".join(lines)


@st.cache_resource(show_spinner=False)
def get_embeddings():
    """ì„ë² ë”© ëª¨ë¸ì„ ë³„ë„ë¡œ ìºì‹±"""
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )


@st.cache_resource(show_spinner=False)
def init_rag_pipeline(google_api_key: str, tavily_api_key: str):
    """PDF(1ìˆœìœ„), ì§€ì •ëœ ì›¹í˜ì´ì§€(2ìˆœìœ„), Tavily ì›¹ê²€ìƒ‰(3ìˆœìœ„)ì„ ì‚¬ìš©í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”."""
    # 1) LLM & ì„ë² ë”©
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.1,
        google_api_key=google_api_key,
    )
    # ì„ë² ë”© ëª¨ë¸ (ë³„ë„ ìºì‹±)
    embeddings = get_embeddings()

    # 2) Tavily ê²€ìƒ‰ ë„êµ¬
    search = TavilySearch(
        max_results=3,
        include_answer=True,
        # ì¼ë°˜ì ìœ¼ë¡œ TAVILY_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
        # ì—¬ê¸°ì„œëŠ” ë³„ë„ api_key ì¸ìë¥¼ ë„˜ê¸°ì§€ ì•ŠìŒ
    )

    # 3) PDF ë²¡í„°ìŠ¤í† ì–´ (ë””ìŠ¤í¬ ìºì‹±)
    pdf_cache_dir = "vectorstore_cache_pdf"

    try:
        if os.path.exists(pdf_cache_dir):
            # ìºì‹œê°€ ìˆìœ¼ë©´ PDF ë¡œë”©/ì²­í‚¹ ìŠ¤í‚µí•˜ê³  ë°”ë¡œ ë¡œë“œ
            pdf_vs = FAISS.load_local(
                pdf_cache_dir,
                embeddings,
                allow_dangerous_deserialization=True,
            )
        else:
            # ìºì‹œê°€ ì—†ì„ ë•Œë§Œ PDF ë¡œë”©/ì²­í‚¹/ë²¡í„°í™” ìˆ˜í–‰
            pdf_docs = load_pdf_docs(PDF_PATH)
            pdf_chunks = split_docs(pdf_docs)
            pdf_vs = FAISS.from_documents(pdf_chunks, embeddings)
            os.makedirs(pdf_cache_dir, exist_ok=True)
            pdf_vs.save_local(pdf_cache_dir)
    except Exception:
        # ìºì‹œê°€ ê¹¨ì¡Œê±°ë‚˜ ë²„ì „ì´ ë‹¬ë¼ì„œ ë¡œë“œ ì‹¤íŒ¨ ì‹œ, ë‹¤ì‹œ ìƒì„±
        pdf_docs = load_pdf_docs(PDF_PATH)
        pdf_chunks = split_docs(pdf_docs)
        pdf_vs = FAISS.from_documents(pdf_chunks, embeddings)
        os.makedirs(pdf_cache_dir, exist_ok=True)
        pdf_vs.save_local(pdf_cache_dir)

    pdf_retriever = pdf_vs.as_retriever(search_kwargs={"k": 6})

    # 4) ì§€ì •ëœ ì›¹í˜ì´ì§€ ë²¡í„°ìŠ¤í† ì–´ (ë””ìŠ¤í¬ ìºì‹±)
    web_cache_dir = "vectorstore_cache_web"
    
    try:
        if os.path.exists(web_cache_dir) and SPECIFIED_URLS:
            web_vs = FAISS.load_local(
                web_cache_dir,
                embeddings,
                allow_dangerous_deserialization=True,
            )
        else:
            # ì›¹í˜ì´ì§€ ë¡œë“œ ë° ë²¡í„°í™”
            if SPECIFIED_URLS:
                web_docs = load_web_docs(SPECIFIED_URLS)
                if web_docs:
                    web_chunks = split_docs(web_docs)
                    web_vs = FAISS.from_documents(web_chunks, embeddings)
                    os.makedirs(web_cache_dir, exist_ok=True)
                    web_vs.save_local(web_cache_dir)
                else:
                    # ì›¹í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                    web_vs = FAISS.from_texts([""], embeddings)
            else:
                # URLì´ ì—†ìœ¼ë©´ ë¹ˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                web_vs = FAISS.from_texts([""], embeddings)
    except Exception as e:
        print(f"Error loading web cache: {e}")
        # ì¬ì‹œë„
        try:
            if SPECIFIED_URLS:
                web_docs = load_web_docs(SPECIFIED_URLS)
                if web_docs:
                    web_chunks = split_docs(web_docs)
                    web_vs = FAISS.from_documents(web_chunks, embeddings)
                    os.makedirs(web_cache_dir, exist_ok=True)
                    web_vs.save_local(web_cache_dir)
                else:
                    web_vs = FAISS.from_texts([""], embeddings)
            else:
                web_vs = FAISS.from_texts([""], embeddings)
        except Exception:
            web_vs = FAISS.from_texts([""], embeddings)

    web_retriever = web_vs.as_retriever(search_kwargs={"k": 6})

    # 5) Tavily ê²€ìƒ‰ â†’ ë¬¸ìì—´ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    def tavily_retrieve(question: str) -> str:
        try:
            result = search.invoke(question)
            return str(result)
        except Exception:
            return ""

    # 6) í”„ë¡¬í”„íŠ¸ (ìš°ì„ ìˆœìœ„ ê·œì¹™ ëª…ì‹œ)
    prompt = ChatPromptTemplate.from_template(
        """
ë‹¹ì‹ ì€ ì—°ì„¸ëŒ€í•™êµ ìš”ëŒ (ëŒ€í•™ êµê³¼ê³¼ì • ì•ˆë‚´ì„œ)ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ,
ì—°ì„¸ëŒ€í•™êµ ìš”ëŒ(PDF), ì§€ì •ëœ ì›¹í˜ì´ì§€, ì¼ë°˜ ì›¹ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì •ë³´ ì‚¬ìš© ìš°ì„ ìˆœìœ„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
1. PDF ìš”ëŒ ë¬¸ì„œ(YonseiUniversityCatalog.pdf)ì˜ ë‚´ìš©
2. ì§€ì •ëœ ì›¹í˜ì´ì§€ì˜ ë‚´ìš©
3. Tavily ì›¹ê²€ìƒ‰ ê²°ê³¼

ê·œì¹™:
- ë¨¼ì € PDF ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì¥ ì‹ ë¢°í•˜ê³ , ëª¨ìˆœë˜ëŠ” ì •ë³´ê°€ ìˆì„ ê²½ìš° PDF ë‚´ìš©ì„ ìš°ì„ í•©ë‹ˆë‹¤.
- PDFì™€ ì§€ì •ëœ ì›¹í˜ì´ì§€ ëª¨ë‘ì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ì„ ë•Œë§Œ Tavily ì›¹ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì¡°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì„¸ ì»¨í…ìŠ¤íŠ¸ ì–´ë””ì—ë„ ê´€ë ¨ ì •ë³´ê°€ ì—†ì§€ë§Œ, ìš”ëŒì´ë‚˜ êµê³¼ê³¼ì •, í•™ì‚¬ì •ë³´ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ëŠ” ê²ƒì€ ë§ë‹¤ê³  íŒë‹¨ë˜ë©´,
  "ì•„ë˜ ë‚´ìš©ì€ ë‚´ì¥ ì •ë³´ì—ì„œ ì°¾ì§€ ëª»í•œ ë‚´ìš©ì´ë¯€ë¡œ ì‹ ë¢°í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¨ìˆœ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì„¸ìš”."ë¼ê³  ë§í•œ ë’¤,
  ì—°ì„¸ëŒ€í•™êµì— í•œí•´ ë¬¼ì–´ë´¤ë‹¤ëŠ” ì „ì œ í•˜ì— ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì´ ìš”ëŒì´ë‚˜ êµê³¼ê³¼ì •, í•™ì‚¬ì •ë³´ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ê³  íŒë‹¨ë˜ë©´, "êµê³¼ì •ë³´ ë° í•™ì‚¬ì •ë³´ì— ê´€í•œ ì§ˆë¬¸ì¸ì§€ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•´ì£¼ì„¸ìš”."ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.
- ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[PDF CONTEXT]
{pdf_context}

[WEB CONTEXT]
{web_context}

[SEARCH CONTEXT]
{search_context}

[ì§ˆë¬¸]
{question}
"""
    )

    # 7) ìµœì¢… ì§ˆì˜ â†’ ë‹µë³€ í•¨ìˆ˜
    # ì²´ì¸ ë°©ì‹ ì‚¬ìš© (ë” ì•ˆì •ì )
    chain = prompt | llm | StrOutputParser()
    
    def answer(question: str) -> str:
        try:
            # ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
            rewrite_prompt = ChatPromptTemplate.from_template(
                """
                ë‹¹ì‹ ì€ ì—°ì„¸ëŒ€í•™êµ ìš”ëŒ(ëŒ€í•™ êµê³¼ê³¼ì • ì•ˆë‚´ì„œ)ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ìš”ëŒ ë¬¸ì„œì—ì„œ ë‹µë³€í•˜ê¸°ì— ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ëª¨í˜¸í•œ ê²½ìš°, 
                ìš”ëŒ ë¬¸ì„œì˜ ë§¥ë½ì— ë§ê²Œ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.
                
                ê·œì¹™:
                - ì›ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ì„¸ìš”
                - ìš”ëŒ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ìœ í˜•(í•™ê³¼, ê³¼ëª©, í•™ì , ì¡¸ì—…ìš”ê±´ ë“±)ì— ë§ê²Œ ìš©ì–´ë¥¼ ì¬êµ¬ì„±í•˜ì„¸ìš”
                - ì—°ì„¸ëŒ€í•™êµ ê´€ë ¨ ë§¥ë½ì„ ëª…í™•íˆ í•˜ì„¸ìš”
                - ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë‹¤ë©´ ì—°ì„¸ëŒ€í•™êµ ìš”ëŒ ë¬¸ì„œì˜ êµ¬ì¡°(í•™ê³¼ë³„, ê³¼ëª©ë³„ ë“±)ì— ë§ê²Œ êµ¬ì²´í™”í•˜ì„¸ìš”
                - ì§ˆë¬¸ì´ ì´ë¯¸ ëª…í™•í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
                
                ì›ë˜ ì§ˆë¬¸: {question}
                
                ì¬êµ¬ì„±ëœ ì§ˆë¬¸:
                """
            )
            
            rewrite_chain = rewrite_prompt | llm | StrOutputParser()
            refined_question = rewrite_chain.invoke({"question": question})
            
            # 1ìˆœìœ„: PDF (ë³´ê°•ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰)
            pdf_docs_rel = pdf_retriever.invoke(refined_question)
            pdf_context = format_docs(pdf_docs_rel)
            
            # 2ìˆœìœ„: ì§€ì •ëœ ì›¹í˜ì´ì§€ (ë³´ê°•ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰)
            web_docs_rel = web_retriever.invoke(refined_question)
            web_context = format_docs(web_docs_rel)
            
            # 3ìˆœìœ„: Tavily ì›¹ê²€ìƒ‰
            search_context = tavily_retrieve(question)
            
            # ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (ë” ì•ˆì •ì )
            result = chain.invoke({
                "pdf_context": pdf_context,
                "web_context": web_context,
                "search_context": search_context,
                "question": question,
            })
            return result
            
        except Exception as e:
            # ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)
            import traceback
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
            return error_msg

    return answer


# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Streamlit Secrets ì§€ì›)
try:
    api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY", None)
    tavily_api_key = os.getenv("TAVILY_API_KEY") or st.secrets.get("TAVILY_API_KEY", None)
except Exception:
    # Streamlit Secretsê°€ ì—†ëŠ” ê²½ìš° (ë¡œì»¬ í™˜ê²½)
    api_key = os.getenv("GOOGLE_API_KEY")
    tavily_api_key = os.getenv("TAVILY_API_KEY")

if not api_key:
    st.error("Error: GOOGLE_API_KEY not found. Please set it in environment variables or Streamlit secrets.")
elif not tavily_api_key:
    st.error("Error: TAVILY_API_KEY not found. Please set it in environment variables or Streamlit secrets.")
else:
    try:
        # RAG íŒŒì´í”„ë¼ì¸ì´ ì•„ì§ ì¤€ë¹„ ì•ˆ ë˜ì—ˆìœ¼ë©´, ë¨¼ì € ë¡œë”© ì „ìš© í™”ë©´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
        if "rag_qa" not in st.session_state:
            # í™”ë©´ ì „ì²´ë¥¼ ë®ëŠ” ë¡œë”© ì˜¤ë²„ë ˆì´ í‘œì‹œ
            st.markdown(
                """<style>
.loading-overlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100vw;
    height: 100vh;
    background: #f5f7fb;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    z-index: 9999;
}
.loading-overlay img {
    width: 220px;
    max-width: 60vw;
}
.loading-overlay .loading-title {
    font-size: 1.9rem;
    font-weight: 700;
    margin-top: 1.5rem;
    margin-bottom: 0.5rem;
    color: #0f1c3f;
}
.loading-overlay .loading-subtitle {
    font-size: 1rem;
    color: #4a4a4a;
    margin-bottom: 1.5rem;
    max-width: 420px;
}
.loading-overlay .loading-indicator {
    font-size: 1rem;
    color: #0f62fe;
    letter-spacing: 0.1rem;
    animation: pulse 1.2s ease-in-out infinite;
}
@keyframes pulse {
    0% { opacity: 0.3; }
    50% { opacity: 1; }
    100% { opacity: 0.3; }
}
</style>
<div class="loading-overlay">
    <img src="https://www.yonsei.ac.kr/sites/sc/images/sub/img-sig8.png" alt="Yonsei Symbol" />
    <div class="loading-title">ì—°ì„¸ëŒ€í•™êµ ìš”ëŒë„ìš°ë¯¸ ìš”ëŒì¡°ëŒ</div>
    <div class="loading-subtitle">ìš”ëŒ PDFë¥¼ ì½ê³  ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. <br>ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.</div>
    <div class="loading-indicator">LOADING...</div>
</div>""",
                unsafe_allow_html=True,
            )

            # ë¦¬ì†ŒìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì„¸ì…˜ì— ì €ì¥
            rag_qa = init_rag_pipeline(api_key, tavily_api_key)
            st.session_state["rag_qa"] = rag_qa

            # ì´ˆê¸°í™”ê°€ ëë‚¬ìœ¼ë‹ˆ ì „ì²´ í™”ë©´ì„ ë‹¤ì‹œ ê·¸ë¦½ë‹ˆë‹¤.
            st.rerun()

        # ì—¬ê¸°ë¶€í„°ëŠ” rag_qaê°€ ì´ë¯¸ ì¤€ë¹„ëœ ìƒíƒœ
        rag_qa = st.session_state["rag_qa"]

        # ìƒë‹¨ í—¤ë”
        col1, col2 = st.columns([1, 5])
        with col1:
            st.image(
                "https://www.yonsei.ac.kr/sites/sc/images/sub/img-symbol6.png", width=75
            )
        with col2:
            st.title("ì—°ì„¸ëŒ€ ìš”ëŒë„ìš°ë¯¸ :blue[ìš”ëŒì¡°ëŒ]")
            st.markdown("#### ìš°ë¦¬ëŒ€í•™ ìš”ëŒì„ ìš”ëª©ì¡°ëª©! \nìš”ëŒ ë° êµê³¼ì •ë³´, ì „ê³µ ë° í•™ì‚¬ì •ë³´ì— ì‹¤ì‹œê°„ ë‹µë³€í•´ì£¼ëŠ” ì—°ì„¸ëŒ€ ì „ìš© ì±—ë´‡ì…ë‹ˆë‹¤.")

        # ì‚¬ì´ë“œë°” ê³ ì • ë„ˆë¹„
        st.markdown(
            """
            <style>
            [data-testid="stSidebar"] {
                width: 365px !important;
            }
            </style>
            """,
            unsafe_allow_html=True,
        )

        # ì‚¬ì´ë“œë°”
        with st.sidebar:
            st.header("ğŸ’¬ ëŒ€í™” ê´€ë¦¬\n")
            st.header("")

            if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            st.markdown("---")
            st.markdown(
                '''
            **ğŸ” ì´ ì±—ë´‡ì€:**
            - ì—°ì„¸ëŒ€ ìš”ëŒPDF ë° í™ˆí˜ì´ì§€ ê¸°ë°˜ ë‹µë³€ ğŸ“˜
            - í•„ìš”í•œ ì „ê³µ, ê³¼ëª©, êµê³¼ì´ìˆ˜, ê°ì¢… ì œë„ ë“±ì— ê´€í•œ ëª¨ë“  ì§ˆë¬¸ ë‹µë³€ ê°€ëŠ¥ ğŸ’¡
            - ìƒë‹´ì„¼í„°ì˜ ë‹µë³€ì„ ê¸°ë‹¤ë¦´ í•„ìš”ì—†ì´ ì•½ 10ì´ˆë§Œì— ì‹¤ì‹œê°„ ë‹µë³€ â±ï¸

            **âœï¸ ì§ˆë¬¸ ê°€ì´ë“œ:**
            - í•™ì‚¬ì¼ì •, ê³¼ëª©ë³„ ê°œì„¤ì—¬ë¶€ ë° ì¼ì •ì´ë‚˜ êµìˆ˜ë‹˜ ì •ë³´ì™€ ê°™ì´
              í•™ê¸°ë³„ë¡œ ë³€ë™ë˜ëŠ” ì •ë³´ëŠ” ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤.
            - ê³¼ëª© ëª©ë¡ì„ ì•Œê³  ì‹¶ì„ ë•Œì—ëŠ” í‘œë¥¼ ìš”ì²­í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
              ì˜ˆì‹œ) "xxê³¼ ì „ê³µìˆ˜ì—… ëª©ë¡ì„ í‘œë¡œ ì •ë¦¬í•´ì¤˜."
            - ì§ˆë¬¸ì€ ëª…ë£Œí•˜ê³  êµ¬ì²´ì ì¼ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.
            '''
            )

        # Session State ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ì§€ê¸ˆê¹Œì§€ ëŒ€í™” ì¶œë ¥
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                avatar = "ğŸ‘¤"
            else:
                avatar = "https://www.yonsei.ac.kr/sites/sc/images/sub/img-symbol6.png"

            with st.chat_message(msg["role"], avatar=avatar):
                st.write(msg["content"])

        # ì‚¬ìš©ì ì…ë ¥
        user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥/í‘œì‹œ
            st.session_state.messages.append(
                {"role": "user", "content": user_input}
            )
            with st.chat_message("user", avatar="ğŸ‘¤"):
                st.write(user_input)

            # RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
            with st.chat_message(
                "assistant",
                avatar="https://www.yonsei.ac.kr/sites/sc/images/sub/img-symbol6.png",
            ):
                with st.spinner("ìƒê° ì¤‘..."):
                    try:
                        assistant_message = rag_qa(user_input)
                    except Exception as e:
                        assistant_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                    st.write(assistant_message)

            # assistant ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append(
                {"role": "assistant", "content": assistant_message}
            )

    except Exception as e:
        st.error(f"An error occurred: {e}")